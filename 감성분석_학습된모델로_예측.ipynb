{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "감성분석_학습된모델로 예측.ipynb",
      "provenance": [],
      "mount_file_id": "1KRIpTZpQMo2Az5kgaNn0JaXaptVvTZDx",
      "authorship_tag": "ABX9TyO98Wp3L3auZ4kAxoV3yClI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagyeom23658/project_dayeom_chatbot/blob/main/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D_%ED%95%99%EC%8A%B5%EB%90%9C%EB%AA%A8%EB%8D%B8%EB%A1%9C_%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install konlpy"
      ],
      "metadata": {
        "id": "Q3b-C8rFjbk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "id": "8n7xHV7Mj3_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "6MQlmhT4jb9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads=8, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim # d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert embedding_dim % self.num_heads == 0\n",
        "\n",
        "        self.projection_dim = embedding_dim // num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embedding_dim': self.embedding_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'projection_dim': self.projection_dim,\n",
        "            'query_dense': self.query_dense,\n",
        "            'key_dense': self.key_dense,\n",
        "            'self.value_dense': self.value_dense,\n",
        "            'self.dense': self.dense,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value):\n",
        "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        logits = matmul_qk / tf.math.sqrt(depth)\n",
        "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # (batch_size, seq_len, embedding_dim)\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        query = self.split_heads(query, batch_size)  \n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
        "        # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "        # (batch_size, seq_len, embedding_dim)\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
        "        outputs = self.dense(concat_attention)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "tuUDcHxaK4WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim2, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embedding_dim2=embedding_dim2\n",
        "        self.num_heads = num_heads\n",
        "        self.dff=dff\n",
        "        self.att = MultiHeadAttention(embedding_dim2, num_heads)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
        "             tf.keras.layers.Dense(embedding_dim2),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'embedding_dim2': self.embedding_dim2,\n",
        "            'num_heads' : self.num_heads,\n",
        "            'dff' : self.dff,\n",
        "            'att': self.att,\n",
        "            'ffn': self.ffn,\n",
        "            'layernorm1': self.layernorm1,\n",
        "            'layernorm2': self.layernorm2,\n",
        "            'dropout1': self.dropout1,\n",
        "            'dropout2': self.dropout2,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n",
        "        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output) # Add & Norm\n"
      ],
      "metadata": {
        "id": "6dmMn3ggLbjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embedding_dim2, **kwargs):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.max_len = max_len   # 아래 config떄문에 이 부분도 추가하고...\n",
        "        self.vocab_size = vocab_size    #\n",
        "        self.embedding_dim2= embedding_dim2       #\n",
        "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim2)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'max_len':self.max_len,          # 이 부분에서 config에 max_len, vocab_size, embedding_dim을 추가해주지 않아서 모델로딩이 계속 안되었던것 같은데(추측) \n",
        "            'vocab_size': self.vocab_size,       #  TypeError: __init__() missing 3 required positional arguments: 'max_len', 'vocab_size', and 'embedding' \n",
        "            'embedding_dim2':self.embedding_dim2,          # 그런데, 'embedding_dim'을 추가하려했더니 계속 이미 config목록에 있다고 추가가 안되서 키값을 바꿔서 넣어줬다. \n",
        "            'token_emb': self.token_emb,            # 모델 로드할 떄 이게 왜 필요한지 모르겠다. \n",
        "            'pos_emb': self.pos_emb,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, x):\n",
        "        max_len = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "6E4k0zG4Lexw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word = pd.read_excel('/content/drive/MyDrive/프로젝트1/stop_words.xlsx',header=None) \n",
        "stop_words=set(stop_word.iloc[:,0].values.tolist())"
      ],
      "metadata": {
        "id": "E4t-eCjskF4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feel_bic_dic = {'기쁨': 0, '당황': 2, '분노': 4, '불안': 1, '상처': 5, '슬픔': 3}\n",
        "feel_bic_dic_reverse = {0: '기쁨', 1: '불안', 2: '당황', 3: '슬픔', 4: '분노', 5: '상처'}\n",
        "model = tf.keras.models.load_model('feel_analysis_model.h5')"
      ],
      "metadata": {
        "id": "CNOl4ZPsjdk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측해보기\n",
        "from hanspell import spell_checker\n",
        "okt=Okt() \n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def sentiment_predict(new_sentence):\n",
        "\n",
        "  spelled_sent = spell_checker.check(new_sentence)    # 챗봇에 단어를 입력할 때는 비문법이 많으므로 문법을 맞춰주고 띄어쓰기를 시켜준다. 이걸 추가하니까 성능이 훨씬 좋아졌다.\n",
        "  hanspell_sent = spelled_sent.checked\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', hanspell_sent)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stop_words] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = 25) # 패딩\n",
        "  score = model.predict(pad_new) # 예측\n",
        "#   print(score[0, score.argmax()])\n",
        "  return feel_bic_dic_reverse[score.argmax()]"
      ],
      "metadata": {
        "id": "wLGyr6i1j02n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predict('상장을 받았어')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AKhy29ZVlS1L",
        "outputId": "a449fe34-4614-4ab5-9f04-4b452c9ab530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'기쁨'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}