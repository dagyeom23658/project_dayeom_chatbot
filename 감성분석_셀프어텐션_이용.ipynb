{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "감성분석_셀프어텐션 이용.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "108W83r9xN7ZUx5ybaeoIAczK9pXG67JU",
      "authorship_tag": "ABX9TyOfJAWP2l8D1cKhqRDmE5zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagyeom23658/project_dayeom_chatbot/blob/main/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D_%EC%85%80%ED%94%84%EC%96%B4%ED%85%90%EC%85%98_%EC%9D%B4%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrEBqOg6KrxB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트렌스포머\n",
        "\n"
      ],
      "metadata": {
        "id": "r-NEjzCGK45P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티헤드어텐션 구현"
      ],
      "metadata": {
        "id": "GDRWEF3WLHmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim # d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert embedding_dim % self.num_heads == 0\n",
        "\n",
        "        self.projection_dim = embedding_dim // num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value):\n",
        "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        logits = matmul_qk / tf.math.sqrt(depth)\n",
        "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # (batch_size, seq_len, embedding_dim)\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        query = self.split_heads(query, batch_size)  \n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
        "        # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "        # (batch_size, seq_len, embedding_dim)\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
        "        outputs = self.dense(concat_attention)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "tuUDcHxaK4WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더 설계\n",
        "- 멀티 헤드 어텐션에 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 추가하여 인코더 클래스를 설계"
      ],
      "metadata": {
        "id": "V27Sgt_6LKKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, dff, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
        "             tf.keras.layers.Dense(embedding_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n",
        "        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output) # Add & Norm"
      ],
      "metadata": {
        "id": "6dmMn3ggLbjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 포지션 임베딩\n",
        "- 포지셔널 인코딩을 사용하였지만, 이번에는 위치 정보 자체를 학습을 하도록 하는 포지션 임베딩이라는 방법을 사용\n",
        "- 포지션 임베딩은 임베딩 층(Embedding layer)를 사용하되, 위치 벡터를 학습하도록 하므로 임베딩 층의 첫번째 인자로 단어 집합의 크기가 아니라 문장의 최대 길이를 넣어줍니다."
      ],
      "metadata": {
        "id": "m1B5GPixLkML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        max_len = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "6E4k0zG4Lexw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6ak_KcwoL9Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLpO8ZqtSAWk",
        "outputId": "10d07b44-b7c8-40a1-f876-c86dcca7f8ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 11.8 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm  #상태진행률 표시\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "zuCdUSm6SFh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_excel('/content/drive/MyDrive/프로젝트1/감성대화말뭉치(최종데이터)_Training.xlsx')  # 코랩에 올리고 실행되기까지 시간이 좀 걸림.\n",
        "train_data.head(3)"
      ],
      "metadata": {
        "id": "8nwFsCG3SF8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = pd.read_excel('/content/drive/MyDrive/프로젝트1/감성대화말뭉치(최종데이터)_Validation.xlsx')  # 코랩에 올리고 실행되기까지 시간이 좀 걸림.\n",
        "val_data.head(3)"
      ],
      "metadata": {
        "id": "zZckiPHISICq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1 = pd.read_excel('/content/drive/MyDrive/프로젝트1/ko_stop_words.xlsx',header=None) "
      ],
      "metadata": {
        "id": "J66SVrMaSKMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words2 = pd.read_excel('/content/drive/MyDrive/프로젝트1/ko_stop_words2.xlsx', header=None)  # 코랩에 올리고 실행되기까지 시간이 좀 걸림.\n",
        "stop_words2.head()"
      ],
      "metadata": {
        "id": "SVmwl6jtSR83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words1.shape)\n",
        "print(stop_words1.isna().sum())\n",
        "\n",
        "stop1 = stop_words1.iloc[:,0].to_frame().values.tolist()\n",
        "stop2 = stop_words1.iloc[:,1].to_frame().values.tolist()\n",
        "stop3 = stop_words1.iloc[:,2].to_frame().dropna().values.tolist()\n",
        "\n",
        "# 2차원리스트 -> 1차원리스트로 변환 https://codechacha.com/ko/python-flatten-list/\n",
        "\n",
        "stop = stop1+stop2+stop3\n",
        "print(len(stop))\n",
        "\n",
        "stop2=sum(stop,[])\n",
        "print(len(stop2))\n",
        "\n",
        "stop_im = stop_words2[0].to_list()\n",
        "stop_word=stop2 + stop_im\n",
        "print(len(stop_word))\n",
        "\n",
        "stop_words=set(stop_word)\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "6akFQPncSTX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.add(',')\n",
        "stop_words.add('.')"
      ],
      "metadata": {
        "id": "0458hEN-SVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터에 대한 이해"
      ],
      "metadata": {
        "id": "neh4w8PyScw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "kI31AhjdSYMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['번호']=train_data['번호'].astype('str')\n",
        "\n",
        "# 앞뒤 공백 처리.\n",
        "train_data =train_data.apply(lambda x: x.str.strip() , axis = 1)  #https://www.delftstack.com/ko/howto/python-pandas/difference-between-pandas-apply-map-and-applymap/\n",
        "train_data.head(3)"
      ],
      "metadata": {
        "id": "EISFJkw5SmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 감정_대분류의 개수 : {}'.format(len(train_data['감정_대분류'].unique())))\n",
        "print(train_data['감정_대분류'].unique())"
      ],
      "metadata": {
        "id": "z-1of6bKTVKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 감정_소분류의 개수 : {}'.format(len(train_data['감정_소분류'].unique())))\n",
        "print(train_data['감정_소분류'].unique())"
      ],
      "metadata": {
        "id": "dVexfZl-TW__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 감성 레이블 만들기(숫자로 매핑)"
      ],
      "metadata": {
        "id": "EiazGyCbS0CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#감정 대분류와 소분류를 키값쌍으로 정렬.\n",
        "dic = {}\n",
        "for uni_class in train_data['감정_대분류'].unique():\n",
        "    uni = train_data[train_data['감정_대분류'] == uni_class]['감정_소분류'].unique()\n",
        "    dic[uni_class] = list(uni)\n",
        "print(dic['기쁨'])"
      ],
      "metadata": {
        "id": "f6DYxvjqSsB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정소분류주제 58개를 숫자레이블 딕셔너리로 변경\n",
        "feel_class = train_data['감정_소분류'].unique()\n",
        "\n",
        "i = 0\n",
        "feel_dic = {}\n",
        "for feel in feel_class:\n",
        "    feel_dic[feel] = i\n",
        "    i+=1\n",
        "# feel_dic  #key:value = 감정소분류:숫자"
      ],
      "metadata": {
        "id": "HjZXS0bkTBiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자로 감정을 찾을 수 있도록 reverse\n",
        "feel_dic_reverse = {}\n",
        "for key, value in feel_dic.items():\n",
        "    feel_dic_reverse[value] = key\n",
        "feel_dic_reverse"
      ],
      "metadata": {
        "id": "TG7xTAt-TB2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feel_bic_class = train_data['감정_대분류'].unique()\n",
        "\n",
        "i = 0\n",
        "feel_bic_dic= {}\n",
        "for feel in feel_bic_class:\n",
        "    feel_bic_dic[feel] = i\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "4uRAOxRWUCBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자로 감정을 찾을 수 있도록 reverse\n",
        "feel_bic_dic_reverse = {}\n",
        "for key, value in feel_bic_dic.items():\n",
        "    feel_bic_dic_reverse[value] = key\n",
        "feel_bic_dic_reverse"
      ],
      "metadata": {
        "id": "mPpMJ5iGUCWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feel_bic_df = train_data[['감정_대분류','사람문장1']]\n",
        "feel_bic_df"
      ],
      "metadata": {
        "id": "DIg8ZjHyUKHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li = []\n",
        "for feel in feel_bic_df['감정_대분류']:\n",
        "    li.append(feel_bic_dic[feel])\n",
        "feel_bic_df['대분류_num'] = li\n",
        "feel_bic_df"
      ],
      "metadata": {
        "id": "WZnlw4apUKRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 값의 분포 시각화\n",
        "plt.figure(figsize=(6, 4))\n",
        "feel_bic_df['대분류_num'].value_counts().plot(kind='bar');"
      ],
      "metadata": {
        "id": "GwKES3Y-UNkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('가장많은 분포의 감정: ', feel_bic_dic_reverse[1])\n",
        "print('감정분포 2위: ', feel_bic_dic_reverse[4])\n",
        "print('감정분포 3위: ',feel_bic_dic_reverse[3])\n",
        "print('감정분포 가장적은: ',feel_bic_dic_reverse[2])"
      ],
      "metadata": {
        "id": "85jkniRnUOGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 레이블이 몇개인가\n",
        "print(feel_bic_df.groupby('대분류_num').size().reset_index(name='count')) "
      ],
      "metadata": {
        "id": "PhpbvlZRUVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train, val 전처리 완료 및 레이블 만들기"
      ],
      "metadata": {
        "id": "R_peywZhUb79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 공백제거\n",
        "val_data['번호']=val_data['번호'].astype('str')\n",
        "val_data =val_data.apply(lambda x: x.str.strip() , axis = 1)"
      ],
      "metadata": {
        "id": "appcJgG202t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인\n",
        "val_data.head(3)"
      ],
      "metadata": {
        "id": "HUnCTAy91iWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train셋과 val셋 비교\n",
        "print('총 감정_대분류의 개수 : {}'.format(len(val_data['감정_대분류'].unique())))\n",
        "set(val_data['감정_대분류'].unique()) == set(train_data['감정_대분류'].unique())"
      ],
      "metadata": {
        "id": "6k-MNBYC1qhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(val_data['감정_소분류'].unique()) == set(train_data['감정_소분류'].unique())"
      ],
      "metadata": {
        "id": "rbBKkhSH26dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_label = []\n",
        "for feel in val_data['감정_대분류']:\n",
        "    val_label.append(feel_bic_dic[feel])\n",
        "pd.Series(val_label)"
      ],
      "metadata": {
        "id": "gVwk-HK5TTrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = []\n",
        "for feel in train_data['감정_대분류']:\n",
        "    train_label.append(feel_bic_dic[feel])\n",
        "np.array(train_label)"
      ],
      "metadata": {
        "id": "zC2hVPQUTTrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화"
      ],
      "metadata": {
        "id": "KWZNfYUaU5pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "import re"
      ],
      "metadata": {
        "id": "bIn1KqKnWAcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt()\n",
        "X_train = []\n",
        "for sentence in train_data['사람문장1']:\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stop_words] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)\n",
        "print(X_train[:3])"
      ],
      "metadata": {
        "id": "MaRXng4IWj2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "for sentence in val_data['사람문장1']:\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stop_words] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "hGcwEmyfXAuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정수인코딩\n",
        "\n",
        "- 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다."
      ],
      "metadata": {
        "id": "U-f8OUolYaaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "i9wKiiByYj57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(tokenizer.word_index)   # 여기서 oov는 어떻게 되는거지?\n",
        "\n",
        "# 0은 패딩을 위해 비워둠"
      ],
      "metadata": {
        "id": "FwOyBpf7YnOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) +1     # 단어집합의 크기"
      ],
      "metadata": {
        "id": "UEIOpBDUyMWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "ms2gzokoY3Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:3])\n",
        "\n",
        "# 각 샘플 내의 단어들은 각 단어에 대한 정수로 변환된 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "Q4a81hNCZEzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터와 테스트 데이터의 문장과 레이블을 각각 저장합니다.\n",
        "y_train = np.array(train_label)\n",
        "y_test = np.array(val_label)"
      ],
      "metadata": {
        "id": "YxZfgXwG3VQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패딩"
      ],
      "metadata": {
        "id": "L3TxoaaBvZWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('문장의 최대 길이 :',max(len(text) for text in X_train))\n",
        "print('문장의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(text) for text in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "# 전체 데이터의 길이 분포는 대체적으로 약 9내외의 길이를 가지는 것을 볼 수 있습니다. "
      ],
      "metadata": {
        "id": "s-9HR1zpvbFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있습니다. 특정 길이 변수를 max_len으로 정합니다. 대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 몇일까요? 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만듭니다.\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n"
      ],
      "metadata": {
        "id": "VpJJeG1PwJU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 분포 그래프를 봤을 때, max_len = 20이 적당할 것 같습니다. 이 값이 얼마나 많은 리뷰 길이를 커버하는지 확인해봅시다. 채팅 특성상 엄청 긴 문장이 포함되지 않으므로, 최대길이로 모든 데이터를 패딩하면 됨. --> 그러나 별 차이 없었다고 한다.\n",
        "max_len =  18          # 18 에서 99%\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "metadata": {
        "id": "vyAN-qO_wWPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 훈련 데이터 중 약 99%의 리뷰가 18이하의 길이를 가지는 것을 확인했습니다. 모든 샘플의 길이를 18으로 맞추겠습니다.\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "cgm-5dtzwyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련용, 테스트용 뉴스 기사 데이터의 레이블에 원-핫 인코딩을 합니다.\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "GzjFC0-F5Zi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "FvDqgNR5IGG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('전체 데이터의 크기(shape):', X_train.shape)\n",
        "print('레이블 데이터의 크기(shape):', y_train.shape)"
      ],
      "metadata": {
        "id": "7lBI4zLJZL16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "TGI29vNfZSSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원\n",
        "num_heads = 2  # 어텐션 헤드의 수\n",
        "dff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(max_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n",
        "x = transformer_block(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "2ERi8yryaxfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "gRBeUglxbLlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "ev-H7vtMbQTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}